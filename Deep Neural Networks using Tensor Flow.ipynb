{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep and Artifical Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifical Neural networks have many hidden layers of neurons, that only get/receive data from the layers next to them. This makes it a DEEP neural network. More classification requires equal number of neurons whereas binary classification only needs 2 neurons. The hidden layers do not interact with the world outside. The more layers we have, the deeper the network becomes, the more difficult it becomes to train it. Input layer is our input data (columns in the df, called dummy neurons). We add bias neurons (also called dummy neurons) thanks to Rosenbalt. The bias goes to the neurons based on the weights of each neuron. These weights are modified to reduce the error.\n",
    "\n",
    "Examples of ANN: Google Inception and Microsoft ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:08.615832Z",
     "start_time": "2020-04-20T00:28:00.667740Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:08.668824Z",
     "start_time": "2020-04-20T00:28:08.636381Z"
    }
   },
   "outputs": [],
   "source": [
    "# White wine\n",
    "white = pd.read_csv(\"winequality-white.csv\",sep=\";\")\n",
    "#Get files from the web: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
    "\n",
    "# Red wine\n",
    "red = pd.read_csv(\"winequality-red.csv\",sep=\";\")\n",
    "#Get files from the web: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:08.714767Z",
     "start_time": "2020-04-20T00:28:08.683350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print info on white wine\n",
    "\n",
    "print(white.info())\n",
    "\n",
    "# Print info on red wine\n",
    "\n",
    "print(red.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:08.874431Z",
     "start_time": "2020-04-20T00:28:08.725136Z"
    }
   },
   "outputs": [],
   "source": [
    "white.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:09.087840Z",
     "start_time": "2020-04-20T00:28:08.882241Z"
    }
   },
   "outputs": [],
   "source": [
    "red.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:09.132299Z",
     "start_time": "2020-04-20T00:28:09.100440Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.isnull(white).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:09.153361Z",
     "start_time": "2020-04-20T00:28:09.139423Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.isnull(red).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:28:09.228071Z",
     "start_time": "2020-04-20T00:28:09.157995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add 'type' column to 'white' with value 0\n",
    "white['type'] = 0\n",
    "\n",
    "# Add 'type' column to 'red' with value 1. Note: the underrepresented class usually gets the code 1\n",
    "red['type'] = 1\n",
    "\n",
    "# Append 'white' to 'red'\n",
    "wines = white.append(red, ignore_index=True)\n",
    "wines.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pair-plot (bi-variate visualization of rows and density) allows us to see whether a particular attribute/variable is a good candidate with which the data can be classified by the ANN. If there is significant overlap in density (number of data points per unit, i.e. length of the attribute), especially if the peaks of the diagonal histograms overlap, the algorithm can't use it to classify, i.e. the attribute/variables is a weak attribute for classification. The attributes should have the two classes be separated significantly for the null hypothesis to be rejected.\n",
    "\n",
    "Good discriminators below are:\n",
    "1. Chloride\n",
    "2. Total sulphur dioxide\n",
    "\n",
    "In ANN, the belief is that individually, the attributes may be weak, but taken together they become strong attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:34.558548Z",
     "start_time": "2020-04-20T00:28:09.261316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "wines.head(50)\n",
    "sns.pairplot(wines, diag_kind = \"kde\", hue = \"type\") #always start analysis of pairplots from the diagonals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we are dealing with deep neural network based models, the first version will be rarely satisfactory. So we tweak the hyperparameters to make it perform the way we expect it to perform. Whenever we work with hyperparameters, its best to have at least 3 datasets:\n",
    "1. Training\n",
    "2. Validation\n",
    "3. Testing\n",
    "\n",
    "In the model.fit section, we specify a validation split.\n",
    "\n",
    "Training and validation datasets are used to tweak the hyperparameters (e.g. k number of neighbours or the depth of a decision tree, which are subsumed within the model) whereas the test dataset is used in the last step to test the performance. This is done to prevent \"data leaks\" which can happen in different ways. One way is when we split the dataset into training and test sets, and use that testing dataset for hyperparameter tuning as well. We will then not have a new dataset for the model to predict that the model has not already experienced. Hyperparameter values in this case are already a function of the test data, much like a leaked exam paper. The model will thus not perform well in the real world. So we tweak hyperparameters on the validation dataset.\n",
    "\n",
    "Sometimes, we do not have enough data to split it into three datasets. Moreover, by spliting datasets (particularly small datasets), we are modifying the distribution, hence introducing bias errors. When the dataset is not big enough to split into three (without introducing bias errors), then we use k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:34.844022Z",
     "start_time": "2020-04-20T00:29:34.569007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import 'train_test_split' from 'sklearn.model_selection'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the data\n",
    "X = wines.iloc[:,0:11]\n",
    "\n",
    "# Specify the target labels and flatten the array\n",
    "\n",
    "y = np.ravel(wines.type) # or np.ravel(wines['type'])\n",
    "#y = wines.type\n",
    "\n",
    "# Split the data up in the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "\n",
    "# To obtain a validation dataset\n",
    "\n",
    "# Split the data up in the train and test sets\n",
    "#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "\n",
    "#X_train, X_train_val, y_train, y_train_val = train_test_split(X_train_val, y_train_val, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:34.857511Z",
     "start_time": "2020-04-20T00:29:34.847761Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:34.876606Z",
     "start_time": "2020-04-20T00:29:34.864220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import 'Standardcaler' from 'sklearn.preprocessing'\n",
    "from sklearn.preprocessing import StandardScaler # Standardizes the data (z-scoring), instead of the raw numbers (X-bar * std)\n",
    "\n",
    "# Define the scaler\n",
    "scaler = StandardScaler().fit(X_train) # Do not apply the standard scaler on the output variables, only input\n",
    "\n",
    "# Scale the train set\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# When you have a validation dataset\n",
    "\n",
    "# Scale the train set\n",
    "#X_train = scaler.transform(X_train)\n",
    "\n",
    "# Scale the validation set\n",
    "#X_train = scaler.transform(X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:34.885132Z",
     "start_time": "2020-04-20T00:29:34.879912Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.size\n",
    "\n",
    "# When you have a validation dataset\n",
    "#X_train_val.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:35.376073Z",
     "start_time": "2020-04-20T00:29:34.905335Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Using Tensorflow Keras instead of the original Keras # Keras comes bundled with tensorflow\n",
    "\n",
    "from tensorflow.keras import Sequential # Sequential = simpler to learn, and functional = more versatile and flexible\n",
    "from tensorflow.keras.layers import Dense # Layer, CNN (Convolutional neural network) also has convolution and pooling layers\n",
    "from keras.layers.advanced_activations import ReLU # can be sigmoid, TanH, etc.\n",
    "from tensorflow.keras.callbacks import EarlyStopping # helps determine the optimal number of epochs\n",
    "# used as follows: callbacks=[EarlyStopping(monitor='val_loss', patience=3)] in model.fit\n",
    "\n",
    "# Define the model architecture\n",
    "\n",
    "# Initialize the constructor aka instantiating\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer (hidden layer)\n",
    "model.add(Dense(30, activation = \"relu\", input_shape = (11,)))\n",
    "#30 is density (# of neurons) and is just random, we have to try out different numbers, but keep it at least equal to the number of attributes in the dataset\n",
    "#11 is for the number of columns (independent attributes)\n",
    "# We can add more layers\n",
    "\n",
    "# Add one hidden layer\n",
    "model.add(Dense(20, activation = \"relu\"))\n",
    "\n",
    "# Add an outpuy layer\n",
    "model.add(Dense(1, activation = \"sigmoid\")) # 1 layer because its a binary classification, sigmoid = logistic regression\n",
    "\n",
    "# Add an input layer\n",
    "#model.add(Dense(10, activation = \"relu\", input_shape = (11,)))\n",
    "\n",
    "# Add an input layer\n",
    "#model.add(Dense(20, activation = \"relu\", input_shape = (11,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To know whether you are overfitting or underfitting, you should run your model against training and validation datasets simultaneously. You can also write your own activation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep the epochs 20\n",
    "2. Keep the batch size 5000\n",
    "3. Check the accuracy (poor accuracy score, discuss)\n",
    "4. Increase epochs to 40\n",
    "5. Check the accuracy (high accuracy score, discuss)\n",
    "6. Reduce batch size to 256\n",
    "7. Reduce epochs to 20\n",
    "8. Check accuracy score (90%+, discuss)\n",
    "9. Add more hidden layers, what is the observation?\n",
    "10. Replace 'relu' with 'sigmoid', what is the observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:35.486812Z",
     "start_time": "2020-04-20T00:29:35.383277Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 50017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate errors from predicted values we get from the model, we define error function as binary_crossentropy. To reduce this error, we use the optimizer adam (or sgd, rmsprop, etc.). In this step the model transforms from a blueprint to a concrete object (think of how re.compile becomes an object when applied). It also automatically adds bias at this step (by deafult its xavier methodology of initializing the weights).\n",
    "\n",
    "Epochs is one full read of the training dataset to estimate the weights based on the error, i.e. one round of forward and backward propagation to minimize the error. We cannot decide the number of epochs before hand, but there is method to prevent wastage of computational resource called 'early stopping'.\n",
    "\n",
    "To prevent outliers from skewing our data, do an unsupervised clustering to see these outliers. Also, hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:44.871964Z",
     "start_time": "2020-04-20T00:29:35.489000Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size,\n",
    "                    epochs=epochs, validation_split=0.3, verbose=True)\n",
    "loss,accuracy = model.evaluate(X_test, y_test,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:44.919792Z",
     "start_time": "2020-04-20T00:29:44.874111Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:45.183289Z",
     "start_time": "2020-04-20T00:29:44.926674Z"
    }
   },
   "outputs": [],
   "source": [
    "print(history.history[\"accuracy\"])\n",
    "print(history.history[\"val_accuracy\"])\n",
    "\n",
    "ta = pd.DataFrame(history.history[\"accuracy\"])\n",
    "va = pd.DataFrame(history.history[\"val_accuracy\"])\n",
    "\n",
    "tva = pd.concat([ta,va], axis=1)\n",
    "\n",
    "tva.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the boxplots of the training and testing don't overlap, it's a clear indication of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:45.444905Z",
     "start_time": "2020-04-20T00:29:45.188171Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:45.458936Z",
     "start_time": "2020-04-20T00:29:45.449460Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:46.116123Z",
     "start_time": "2020-04-20T00:29:45.465296Z"
    }
   },
   "outputs": [],
   "source": [
    "loss,acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.3f\"%acc)\n",
    "print(\"Test Loss: %.3f\"%loss)\n",
    "\n",
    "loss,acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: %.3f\"%acc)\n",
    "print(\"Train Loss: %.3f\"%loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:46.169272Z",
     "start_time": "2020-04-20T00:29:46.124295Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "df_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred, labels=[0,1]),\n",
    "                         columns=['pred:White','pred:Red'],index=['true:White','true:Red'])\n",
    "print(df_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:46.555493Z",
     "start_time": "2020-04-20T00:29:46.186096Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "pyplot.figure(figsize=(10,6))  \n",
    "sns.heatmap(df_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:46.824514Z",
     "start_time": "2020-04-20T00:29:46.558259Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title(\"Loss\")\n",
    "pyplot.plot(history.history[\"loss\"], label = \"Train\")\n",
    "pyplot.legend()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}